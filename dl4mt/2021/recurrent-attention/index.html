<!DOCTYPE html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.51" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://sanjana-moudgalya.github.io/dl4mt/2021/recurrent-attention/"><meta property="og:site_name" content="Evaluation of Machine Translations with Large Language Models"><meta property="og:title" content="Recurrent Attention for Neural Machine Translation"><meta property="og:type" content="article"><meta property="og:image" content="https://sanjana-moudgalya.github.io/"><meta property="og:updated_time" content="2022-09-13T03:45:15.000Z"><meta property="og:locale" content="en-US"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image:alt" content="Recurrent Attention for Neural Machine Translation"><meta property="article:author" content="Jiachen Li"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="Recurrent Attention"><meta property="article:published_time" content="2021-11-29T00:00:00.000Z"><meta property="article:modified_time" content="2022-09-13T03:45:15.000Z"><title>Recurrent Attention for Neural Machine Translation | Evaluation of Machine Translations with Large Language Models</title><meta name="description" content="A Blog for Machine Learning, Natural Language Processing, and Data Mining">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d2025;
      }

      html,
      body {
        background-color: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="stylesheet" href="/assets/style.c54272bd.css">
    <link rel="modulepreload" href="/assets/app.b72466cd.js"><link rel="modulepreload" href="/assets/index.html.05d474af.js"><link rel="modulepreload" href="/assets/_plugin-vue_export-helper.cdc0426e.js"><link rel="modulepreload" href="/assets/index.html.5c017987.js"><link rel="prefetch" href="/assets/index.html.268ddb49.js"><link rel="prefetch" href="/assets/index.html.a6f7ba20.js"><link rel="prefetch" href="/assets/index.html.305dd651.js"><link rel="prefetch" href="/assets/index.html.d4f6bd17.js"><link rel="prefetch" href="/assets/index.html.bf46a35b.js"><link rel="prefetch" href="/assets/index.html.089b8c8c.js"><link rel="prefetch" href="/assets/index.html.c7b5754b.js"><link rel="prefetch" href="/assets/index.html.815bebb7.js"><link rel="prefetch" href="/assets/index.html.dfe7cd19.js"><link rel="prefetch" href="/assets/index.html.40ec772d.js"><link rel="prefetch" href="/assets/index.html.f5199c80.js"><link rel="prefetch" href="/assets/index.html.8451c228.js"><link rel="prefetch" href="/assets/index.html.f5437c7b.js"><link rel="prefetch" href="/assets/index.html.ed697aad.js"><link rel="prefetch" href="/assets/index.html.6390f335.js"><link rel="prefetch" href="/assets/index.html.cbb2b56c.js"><link rel="prefetch" href="/assets/index.html.cd426b8e.js"><link rel="prefetch" href="/assets/index.html.e68753b1.js"><link rel="prefetch" href="/assets/index.html.0d8c5916.js"><link rel="prefetch" href="/assets/index.html.c16adaf9.js"><link rel="prefetch" href="/assets/index.html.3a45d95e.js"><link rel="prefetch" href="/assets/index.html.a87486f1.js"><link rel="prefetch" href="/assets/index.html.8fd2afe3.js"><link rel="prefetch" href="/assets/404.html.50619a54.js"><link rel="prefetch" href="/assets/index.html.09c8363b.js"><link rel="prefetch" href="/assets/index.html.69899862.js"><link rel="prefetch" href="/assets/index.html.9fc605d2.js"><link rel="prefetch" href="/assets/index.html.c310ae00.js"><link rel="prefetch" href="/assets/index.html.c6690c9d.js"><link rel="prefetch" href="/assets/index.html.d1325f90.js"><link rel="prefetch" href="/assets/index.html.30fc159c.js"><link rel="prefetch" href="/assets/index.html.c7d6bb19.js"><link rel="prefetch" href="/assets/index.html.5eda7c46.js"><link rel="prefetch" href="/assets/index.html.ca4bde1f.js"><link rel="prefetch" href="/assets/index.html.45fc4a0a.js"><link rel="prefetch" href="/assets/index.html.a8f7a8ea.js"><link rel="prefetch" href="/assets/index.html.6b055639.js"><link rel="prefetch" href="/assets/index.html.3f8cd287.js"><link rel="prefetch" href="/assets/index.html.15b47f9a.js"><link rel="prefetch" href="/assets/index.html.cc7fa31a.js"><link rel="prefetch" href="/assets/index.html.5f639ed4.js"><link rel="prefetch" href="/assets/index.html.a044d8aa.js"><link rel="prefetch" href="/assets/index.html.b3d965e1.js"><link rel="prefetch" href="/assets/index.html.9c8b156c.js"><link rel="prefetch" href="/assets/index.html.c830c850.js"><link rel="prefetch" href="/assets/index.html.3d9f674f.js"><link rel="prefetch" href="/assets/index.html.844760a8.js"><link rel="prefetch" href="/assets/index.html.48a4c6bb.js"><link rel="prefetch" href="/assets/index.html.e5a8c109.js"><link rel="prefetch" href="/assets/index.html.75e7b8de.js"><link rel="prefetch" href="/assets/index.html.7afc5d3b.js"><link rel="prefetch" href="/assets/index.html.05d23387.js"><link rel="prefetch" href="/assets/index.html.ebc1236d.js"><link rel="prefetch" href="/assets/index.html.c03a858b.js"><link rel="prefetch" href="/assets/index.html.30d725c7.js"><link rel="prefetch" href="/assets/index.html.84ebbdab.js"><link rel="prefetch" href="/assets/index.html.3d799753.js"><link rel="prefetch" href="/assets/index.html.1c375715.js"><link rel="prefetch" href="/assets/index.html.3684b044.js"><link rel="prefetch" href="/assets/index.html.9de46b57.js"><link rel="prefetch" href="/assets/index.html.6142d6fa.js"><link rel="prefetch" href="/assets/index.html.d12f3b0e.js"><link rel="prefetch" href="/assets/index.html.cacab408.js"><link rel="prefetch" href="/assets/index.html.91b18d0d.js"><link rel="prefetch" href="/assets/index.html.6951f4ad.js"><link rel="prefetch" href="/assets/index.html.10864e82.js"><link rel="prefetch" href="/assets/index.html.2e441183.js"><link rel="prefetch" href="/assets/index.html.d37a04d7.js"><link rel="prefetch" href="/assets/index.html.5ecf434f.js"><link rel="prefetch" href="/assets/index.html.1296e2ac.js"><link rel="prefetch" href="/assets/index.html.4f762319.js"><link rel="prefetch" href="/assets/index.html.08fea502.js"><link rel="prefetch" href="/assets/index.html.8f71874e.js"><link rel="prefetch" href="/assets/index.html.080e09c1.js"><link rel="prefetch" href="/assets/index.html.554bfbac.js"><link rel="prefetch" href="/assets/index.html.bcb2beed.js"><link rel="prefetch" href="/assets/index.html.3f0b6056.js"><link rel="prefetch" href="/assets/index.html.388269db.js"><link rel="prefetch" href="/assets/index.html.6f8d6af7.js"><link rel="prefetch" href="/assets/index.html.b6a3de06.js"><link rel="prefetch" href="/assets/index.html.6effd2a6.js"><link rel="prefetch" href="/assets/index.html.4ce339c9.js"><link rel="prefetch" href="/assets/index.html.4a70087b.js"><link rel="prefetch" href="/assets/index.html.c321b5a0.js"><link rel="prefetch" href="/assets/index.html.ed871a0c.js"><link rel="prefetch" href="/assets/index.html.a5f59eb2.js"><link rel="prefetch" href="/assets/index.html.028f858b.js"><link rel="prefetch" href="/assets/index.html.73f1d3ab.js"><link rel="prefetch" href="/assets/index.html.afc21b56.js"><link rel="prefetch" href="/assets/index.html.4b151f61.js"><link rel="prefetch" href="/assets/index.html.38f9c00c.js"><link rel="prefetch" href="/assets/index.html.3dcf322d.js"><link rel="prefetch" href="/assets/index.html.c0f1f166.js"><link rel="prefetch" href="/assets/index.html.bde1e0f6.js"><link rel="prefetch" href="/assets/index.html.4c39aaa0.js"><link rel="prefetch" href="/assets/index.html.f18df93b.js"><link rel="prefetch" href="/assets/index.html.58665236.js"><link rel="prefetch" href="/assets/index.html.ec25d931.js"><link rel="prefetch" href="/assets/index.html.3639f90b.js"><link rel="prefetch" href="/assets/index.html.bcd7ed0f.js"><link rel="prefetch" href="/assets/404.html.fa96082e.js"><link rel="prefetch" href="/assets/index.html.71bdc391.js"><link rel="prefetch" href="/assets/index.html.72e1ba8f.js"><link rel="prefetch" href="/assets/index.html.f895ff56.js"><link rel="prefetch" href="/assets/index.html.21ee6076.js"><link rel="prefetch" href="/assets/index.html.5982b3db.js"><link rel="prefetch" href="/assets/index.html.80d58faa.js"><link rel="prefetch" href="/assets/index.html.dfce0f25.js"><link rel="prefetch" href="/assets/index.html.3683fa1a.js"><link rel="prefetch" href="/assets/index.html.deb46957.js"><link rel="prefetch" href="/assets/index.html.05156901.js"><link rel="prefetch" href="/assets/index.html.50ede47c.js"><link rel="prefetch" href="/assets/index.html.71ec7780.js"><link rel="prefetch" href="/assets/index.html.6d76c62d.js"><link rel="prefetch" href="/assets/index.html.93df525c.js"><link rel="prefetch" href="/assets/index.html.2ac81c4e.js"><link rel="prefetch" href="/assets/index.html.efcf3ba4.js"><link rel="prefetch" href="/assets/index.html.a643e250.js"><link rel="prefetch" href="/assets/index.html.08b2b446.js"><link rel="prefetch" href="/assets/index.html.c348e7ce.js"><link rel="prefetch" href="/assets/index.html.357100f5.js"><link rel="prefetch" href="/assets/index.html.73e30e03.js"><link rel="prefetch" href="/assets/index.html.cc78b290.js"><link rel="prefetch" href="/assets/index.html.3300b79d.js"><link rel="prefetch" href="/assets/index.html.d3edbabf.js"><link rel="prefetch" href="/assets/index.html.6f74217e.js"><link rel="prefetch" href="/assets/index.html.3835edaf.js"><link rel="prefetch" href="/assets/index.html.912ac187.js"><link rel="prefetch" href="/assets/index.html.83e5b6ef.js"><link rel="prefetch" href="/assets/index.html.4b55791b.js"><link rel="prefetch" href="/assets/index.html.17689ce2.js"><link rel="prefetch" href="/assets/index.html.82e352b3.js"><link rel="prefetch" href="/assets/index.html.b3d4400b.js"><link rel="prefetch" href="/assets/index.html.db9dcf73.js"><link rel="prefetch" href="/assets/index.html.3abe77ad.js"><link rel="prefetch" href="/assets/index.html.25b9205b.js"><link rel="prefetch" href="/assets/index.html.22dfffa1.js"><link rel="prefetch" href="/assets/index.html.5a838630.js"><link rel="prefetch" href="/assets/index.html.9826fdc2.js"><link rel="prefetch" href="/assets/index.html.13bc8420.js"><link rel="prefetch" href="/assets/index.html.43995159.js"><link rel="prefetch" href="/assets/index.html.d5e2f85b.js"><link rel="prefetch" href="/assets/index.html.38aa9bb4.js"><link rel="prefetch" href="/assets/index.html.1bd1ee16.js"><link rel="prefetch" href="/assets/index.html.f029f203.js"><link rel="prefetch" href="/assets/index.html.62cd87c4.js"><link rel="prefetch" href="/assets/index.html.f1bca6e0.js"><link rel="prefetch" href="/assets/index.html.f06e3728.js"><link rel="prefetch" href="/assets/index.html.a67e480a.js"><link rel="prefetch" href="/assets/index.html.57ba1cd5.js"><link rel="prefetch" href="/assets/index.html.e9996c26.js"><link rel="prefetch" href="/assets/index.html.94849c58.js"><link rel="prefetch" href="/assets/index.html.b5693fcd.js"><link rel="prefetch" href="/assets/index.html.7065583d.js"><link rel="prefetch" href="/assets/giscus.15440425.js"><link rel="prefetch" href="/assets/highlight.esm.d982e650.js"><link rel="prefetch" href="/assets/markdown.esm.832a189d.js"><link rel="prefetch" href="/assets/math.esm.a3f84b6f.js"><link rel="prefetch" href="/assets/notes.esm.3c361cb7.js"><link rel="prefetch" href="/assets/reveal.esm.b96f05d8.js"><link rel="prefetch" href="/assets/search.esm.80da4a02.js"><link rel="prefetch" href="/assets/zoom.esm.8514a202.js"><link rel="prefetch" href="/assets/photoswipe.esm.382b1873.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">Skip to content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><!--[--><header class="navbar"><div class="navbar-left"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><a href="/" class="brand"><img class="logo" src="/logo.svg" alt="Evaluation of Machine Translations with Large Language Models"><!----><span class="site-name hide-in-pad">Evaluation of Machine Translations with Large Language Models</span></a><!----></div><div class="navbar-center"><!----><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/" class="nav-link" aria-label="Blog Home"><span class="icon iconfont icon-home"></span>Blog Home<!----></a></div><div class="nav-item hide-in-mobile"><a href="/category/" class="nav-link" aria-label="Category"><span class="icon iconfont icon-categoryselected"></span>Category<!----></a></div><div class="nav-item hide-in-mobile"><a href="/tag/" class="nav-link" aria-label="Tags"><span class="icon iconfont icon-tag"></span>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a href="/timeline/" class="nav-link" aria-label="Timeline"><span class="icon iconfont icon-time"></span>Timeline<!----></a></div></nav><!----></div><div class="navbar-right"><!----><!----><div class="nav-item"><a class="repo-link" href="https://github.com/sanjana-moudgalya/mnlp_blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!----><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow left"></span></div><aside class="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->Recurrent Attention for Neural Machine Translation</h1><div class="page-info"><span class="author-info" aria-label="AuthorðŸ–Š" data-balloon-pos="down" localizeddate="November 29, 2021" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="author-item">Jiachen Li</span></span><span property="author" content="Jiachen Li"></span></span><!----><span class="date-info" aria-label="Writing DateðŸ“…" data-balloon-pos="down" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span>November 29, 2021</span><meta property="datePublished" content="2021-11-29T00:00:00.000Z"></span><span class="category-info" aria-label="CategoryðŸŒˆ" data-balloon-pos="down" localizeddate="November 29, 2021" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><ul class="categories-wrapper"><li class="category category4 clickable" role="navigation">MT</li><li class="category category1 clickable" role="navigation">DL4MT</li><meta property="articleSection" content="MT,DL4MT"></ul></span><span aria-label="TagðŸ·" data-balloon-pos="down" localizeddate="November 29, 2021" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><ul class="tags-wrapper"><li class="tag tag1 clickable" role="navigation">Transformer</li><li class="tag tag4 clickable" role="navigation">Recurrent Attention</li></ul><meta property="keywords" content="Transformer,Recurrent Attention"></span><span class="reading-time-info" aria-label="Reading TimeâŒ›" data-balloon-pos="down" localizeddate="November 29, 2021" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 5 min</span><meta property="timeRequired" content="PT5M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><div class="toc-header">On This Page</div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/dl4mt/2021/recurrent-attention/#introduction" class="router-link-active router-link-exact-active toc-link level2">Introduction</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/dl4mt/2021/recurrent-attention/#multi-head-attention-module" class="router-link-active router-link-exact-active toc-link level2">Multi-head Attention Module</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/dl4mt/2021/recurrent-attention/#problem-associated-with-the-self-attention" class="router-link-active router-link-exact-active toc-link level2">Problem Associated with the Self-attention</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/dl4mt/2021/recurrent-attention/#ran-recurrent-attention" class="router-link-active router-link-exact-active toc-link level2">RAN: Recurrent Attention</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/dl4mt/2021/recurrent-attention/#effectiveness-and-analysis-of-the-ran" class="router-link-active router-link-exact-active toc-link level2">Effectiveness and Analysis of the RAN.</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/dl4mt/2021/recurrent-attention/#_1-main-results" class="router-link-active router-link-exact-active toc-link level3">1. Main results</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/dl4mt/2021/recurrent-attention/#_2-analysis" class="router-link-active router-link-exact-active toc-link level3">2. Analysis</a></li><!----><!--]--></ul><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/dl4mt/2021/recurrent-attention/#summary" class="router-link-active router-link-exact-active toc-link level2">Summary</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/dl4mt/2021/recurrent-attention/#references" class="router-link-active router-link-exact-active toc-link level2">References</a></li><!----><!--]--></ul></div></aside></div><!----><div class="theme-hope-content"><p>â€‹Upon its emergence, the Transformer Neural Networks [1] dominates the sequence-to-sequence tasks. It even outperforms the Google Neural Machine Translation model in specific tasks. Specifically, the multi-head attention mechanism that depends on element-wise dot-product is deemed as one of the critical building blocks to get things to work. But is it really that important?</p><!-- more --><p>Reading Time: About 10 minutes.</p><p>Paperï¼š<a href="https://aclanthology.org/2021.emnlp-main.258/" target="_blank" rel="noopener noreferrer">https://aclanthology.org/2021.emnlp-main.258/<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Github: <a href="https://github.com/lemon0830/RAN" target="_blank" rel="noopener noreferrer">https://github.com/lemon0830/RAN<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><p>A recent work that appeared at the 2021 Conference of Empirical Methods in Natural Language Processing, dives into analyzing the efficacy of the dot-product self-attention module. As recent research has shown that most attention heads only learn simple positional patterns, this paper steps further towards this line and propose a novel substitute mechanism for self-attention: Recurrent AtteNtion (RAN).</p><p>The basic idea of RAN is to directly learn attention weights without any token-to-token interaction and perform layer-to-layer interaction. By performing a massive number of experiments on 10 machine translation tasks, this paper empirically proves that the RAN models are competitive and outperform their Transformer counterparts in certain scenarios, with fewer parameters and inference time. Specifically, applying RAN to the decoder of Transformer yields consistent improvements by about +0.5 BLEU on 6 translation tasks and +1.0 BLEU on Turkish-English translation tasks.</p><p>This blog post is organized in the following way: 1) Brief introduction of the Transformer Neural Network, with a focus on the basics of the multi-head self-attention module 2) Problem associated with the self-attention module 3) The solution provided by the RAN mechanism 4) The performance and analysis of RAN.</p><h2 id="multi-head-attention-module" tabindex="-1"><a class="header-anchor" href="#multi-head-attention-module" aria-hidden="true">#</a> Multi-head Attention Module</h2><p><img src="/assets/transformer.e3c7f3ea.png" alt="image1"></p><p>The figure above gives an overview of the Transformer Architecture. The left-hand side provides the encoder architecture, while the right-hand side gives the decoder architecture. Both of the encoder and decoder are stacked by N sub-layers, and the multi-head attention module is the main component in both the encoder and decoder layer. The encoder encodes the inputs and generates the context vector, which serves as an input to the decoder for decoding the output sequences. We refer the interested readers to the original paper [1] and only focus on the Multi-Head Attention module in this article.</p><p><img src="/assets/multi_head_attention.f0d433d6.png" alt="image2"></p><p>The figure above depicts the computation of the dot-product self-attention of the <code>k</code>-th head in the <code>l</code>-th encoder layer. Given a sequence of token representations with a length of <code>n</code>, the self-attention model first converts the representations into three matrice <code>Q</code>, <code>K</code> and <code>V</code>, representing queries, keys, and values, respectively. And <code>d_k</code> is the dimensionality of the vector in the <code>k</code>-th head. Then, the attention matrix is calculated via the dot product of queries and keys followed by rescaling:</p><p><img src="/assets/dot_product.ad410765.png" alt="image3"></p><p>Finally, a softmax operation is applied on this unnormalized attention matrix, and then the output is used to compute a weighted sum of values:</p><p><img src="/assets/eq_2.d1266654.png" alt="image4"></p><p>where <code>H</code> is a new contextual representation of the <code>l</code>-th layer. This procedure can be implemented with a multi-head mechanism by projecting the input into different subspaces, which requires extra splitting and concatenation operations. The output is fed into a position-wise feed-forward network to get the final representations of this layer.</p><h2 id="problem-associated-with-the-self-attention" tabindex="-1"><a class="header-anchor" href="#problem-associated-with-the-self-attention" aria-hidden="true">#</a> Problem Associated with the Self-attention</h2><p>While flexible, it has been proven that there exists redundant information with pair-wise calculation. Many studies have shown that pairwise self-attention is over-parameterized, leading to a costly inference [2, 3, 4]. The RAN method takes this direction to an extreme by showing that self-attention is empirically replaceable. And next, we will formally introduce the RAN method.</p><h2 id="ran-recurrent-attention" tabindex="-1"><a class="header-anchor" href="#ran-recurrent-attention" aria-hidden="true">#</a> RAN: Recurrent Attention</h2><p>RAN consists of a set of global <code>Initial Attention Matrices</code> and a <code>Recurrent Transition Module</code>. Instead of computing the attention weights on the fly as in the original multi-head attention module in each layer, RAN directly learn the attention weights, denoted as</p><p><img src="/assets/initial_weight_matrix.08027d33.png" alt="image6"></p><p>which are exactly the so-called <code>Initial Attention Matrices</code>. Here <code>h</code> denotes the number of heads. On the other hand, the <code>Recurrent Transition Module</code> takes the set of <code>A0</code> as input, and recursively updates the attention matrices layer by layer. Note that the <code>Initial Attention Matrices</code>, the <code>Recurrent Transition Module</code>, and the other modules are optimized jointly. The attention matrices are completely agnostic to the input representations and can be retrieved directly without recomputation during inference.</p><p><img src="/assets/ran.e949f9e3.png" alt="image7"></p><p>Figure above gives the model architecture of the RAN, where the dotted line denotes parameter sharing. It also shows the computation of the <code>k</code>-th head in the <code>l</code>-th encoder layer. The recurrent transition module obtains the attention weights in <code>l</code>-th layer <code>Rec(âˆ—)</code> with the attention matrix from the last layer.</p><p>Moreover, the <code>Recurrent Transition Module</code> is implemented using a single feed-forward network with tanh as its activation function followed by a layer normalization and a residual connection:</p><p><img src="/assets/recurrent_transition.1f2e5238.png" alt="image8"></p><p>Notably, the parameters of the transition module are shared across all heads and all layers.</p><h2 id="effectiveness-and-analysis-of-the-ran" tabindex="-1"><a class="header-anchor" href="#effectiveness-and-analysis-of-the-ran" aria-hidden="true">#</a> Effectiveness and Analysis of the RAN.</h2><h3 id="_1-main-results" tabindex="-1"><a class="header-anchor" href="#_1-main-results" aria-hidden="true">#</a> 1. Main results</h3><p>The original paper evaluates RAN on WMT and NIST translation tasks, including 10 different language pairs altogether. Besides, the authors tried to apply RAN to the encoder (RAN-E), the decoder (RAN-D), or both of them (RAN-ALL), respectively. They compare against the standard Transformer (TransF) [1], and the two most related works are Hard-coded Transformer (HCSA) [5] and Random Synthesizer. (Syn-R) [6].</p><p>Table 1 shows the overall results on the ten language pairs. Compared with TransF, the RAN models consistently yield competitive or even better results against TransF on all datasets. Concretely, 0.13/0.16, 0.48/0.44, and 0.16/0.22 more average BLEU/SacreBLEU are achieved by RAN-E, RAND, and RAN-ALL, respectively. Although different languages have different linguistic and syntactic structures, RAN can learn reasonable global attention patterns over the whole training corpus.</p><p><img src="/assets/main_result.d630b0f7.png" alt="image9"></p><p>Interestingly, RAN-D performs best, which significantly outperforms the TransF on most language pairs. The biggest performance gain comes from the low resource translation task Trâ‡’En where RAN-D outperforms TransF by 0.97/1.0 BLEU/SacreBLEU points. We conjecture that the position-based attention without tokenwise interaction is easier to learn and the RAN can capture more generalized attention patterns. By contrast, the dot-product self-attention is forced to learn the semantic relationship between tokens and may fall into sub-optimal local minima, especially when the training scale is low. In brief, the improvement indicates that NMT systems can benefit from simplified decoders when training data is insufficient. Besides, although both RAN-E and RAN-D are effective, their effects can not be accumulated.</p><p>Moreover, we can see that RAN-ALL vastly outperforms the other two related methods. RAN bridges the performance gap between Transformer and the models without the dot-product self-attention, demonstrating the effectiveness of RAN. And from the figure below, we can see that RAN-ALL successfully speeds up the inference phase.</p><p><img src="/assets/speed_up.08872f1b.png" alt="image10"></p><h3 id="_2-analysis" tabindex="-1"><a class="header-anchor" href="#_2-analysis" aria-hidden="true">#</a> 2. Analysis</h3><p>The figure below visualizes the attention patterns of RAN over positions</p><p><img src="/assets/visual_attention.2f673af7.png" alt="image11"></p><p>We find that in the encoder, RAN focuses its attention on a local neighborhood around each position. Specifically, in the last layer of the encoder, the weights become more concentrated, potentially due to the hidden representations being contextualized. Interestingly, except attending local windows to the current position, the decoder weights are most concentrated in the first token of target sequences. This may demonstrate the mechanism of decoder self-attention that the RAN decoder attends to source-side hidden states based on global source sentence representations aggregated by the start tokens.</p><p><img src="/assets/visual_layer.2d66c0a0.png" alt="image12"></p><p>The figure above depicts the Jensen-Shannon divergence of attention between each pair of layers. The conclusions are as follows: First, the attention similarity in TransF is not salient, but the attention distribution of adjacent layers is similar to some extent. Second, there are no noticeable patterns found in Syn-R. Third, as for RAN-ALL, the attention similarity is high, especially in the decoder (the JS-divergence ranges from 0.08 to 0.2), and is remarkable between adjacent layers.</p><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary" aria-hidden="true">#</a> Summary</h2><p>The RAN architecture is proposed to simplify the Transformer architecture for Neural Machine Translation without costly dot-product self-attention. It takes the <code>Initial Attention Matrices</code> as a whole and updates it by a <code>Recurrent Transition Module recurrently</code>. Experiments on ten representative translation tasks show the effectiveness of RAN.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><p>[1] Vaswani, Ashish, et al. &quot;Attention is all you need.&quot; Advances in neural information processing systems. 2017.</p><p>[2] Sanh, Victor, et al. &quot;DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.&quot; arXiv preprint arXiv:1910.01108 (2019).</p><p>[3] Correia, GonÃ§alo M., et al. &quot;Adaptively sparse transformers.&quot; arXiv preprint arXiv:1909.00015 (2019).</p><p>[4] Xiao, Tong, et al. Sharing attention weights for fast transformer. In Proceedings of IJCAI 2019, pages 5292â€“5298.</p><p>[5] You, Weiqiu, et al. Hard-coded gaussian attention for neural machine translation. In Proceedings of ACL 2020, pages 7689â€“7700.</p><p>[6] Tay, Yi, et al. &quot;Synthesizer: Rethinking self-attention for transformer models.&quot; International Conference on Machine Learning. PMLR, 2021.</p></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/sanjana-moudgalya/mnlp_blog/edit/main/dl4mt/2021/recurrent-attention/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item update-time"><span class="label">Last update: </span><span class="info">9/13/2022, 3:45:15 AM</span></div><div class="meta-item contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: lileicc@gmail.com">Lei Li</span><!--]--><!--]--></div></footer><!----><div class="giscus-wrapper input-top" style="display:block;"><div style="text-align:center">Loading...</div></div><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer">Li Lab</div><div class="copyright">Copyright Â© 2023 Jiachen Li</div></footer><!--]--></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app.b72466cd.js" defer></script>
  </body>
</html>
