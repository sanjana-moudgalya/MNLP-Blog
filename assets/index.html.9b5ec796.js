import{_ as s}from"./_plugin-vue_export-helper.cdc0426e.js";import{o as i,c as r,a as o,b as e,d as n,e as a,f as l,r as h}from"./app.44f61ce1.js";const c={},u=e("h1",{id:"evaluation-of-machine-translations-using-large-language-models",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#evaluation-of-machine-translations-using-large-language-models","aria-hidden":"true"},"#"),a(" Evaluation of Machine Translations using Large Language Models")],-1),d=e("h3",{id:"authors-anusha-rao-anushasa-sanjana-moudgalya-smoudgal",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#authors-anusha-rao-anushasa-sanjana-moudgalya-smoudgal","aria-hidden":"true"},"#"),a(" Authors: Anusha Rao (anushasa), Sanjana Moudgalya (smoudgal)")],-1),g=e("p",null,"To all those tech enthusiasts out there, here\u2019s yet another advancement of Large Language Models (LLM) that\u2019s worth a read. This article explores how powerful these models are in language translation tasks by presenting a summary of evaluations on 8 LLMs including but not limited to Chat-GPT, GPT-4, and Google Translate.",-1),p=a("Paper: "),m={href:"https://arxiv.org/pdf/2304.04675.pdf",target:"_blank",rel:"noopener noreferrer"},f=a("https://arxiv.org/pdf/2304.04675.pdf"),b=a("Code: "),y={href:"https://github.com/NJUNLP/MMT-LLM",target:"_blank",rel:"noopener noreferrer"},L=a("https://github.com/NJUNLP/MMT-LLM"),v=l('<h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><p><img src="https://github.com/sanjana-moudgalya/mnlp_blog/blob/main/MMT-LLM/Introduction.png?raw=true" alt="alt text"> Effective communication is universally recognized as a key to success. But what about the challenges posed by linguistic barriers? Imagine being unable to articulate your thoughts without understanding the language of the person you&#39;re talking to. Wouldn&#39;t it be incredible if language was no longer a barrier, enabling seamless communication with anyone across the globe? Sounds crazy, right? But might not be that far away from reality [Figure 1], and all credit to the advancement in Large Language Models(LLMs).</p><p>Before we delve deeper into translation tasks, let&#39;s first understand what LLMs are. Although you might not be familiar with their workings or design, chances are you&#39;ve encountered them in various forms. For instance, Chat-GPT - does this sound familiar? They are smart systems powered by AI that have learned from heaps of text from all around the world, making them one of the most powerful models out there [Figure 2]. Their training corpus consists of multiple languages, allowing them to master not only their grammar but also the ability to understand the semantics of a language. Thus making them ideal for Multilingual Machine Translation (MMT) tasks.</p><p><img src="https://github.com/sanjana-moudgalya/mnlp_blog/blob/main/MMT-LLM/LLMs.png?raw=true" alt="alt text"> Now that we know what these models are capable of, let&#39;s evaluate their true abilities in language translation. Based on the base paper, we attempt to answer two main questions. Firstly, how effectively are LLMs able to translate text across numerous languages? For languages like English, it is very easy to find training data since it is widely spoken. But what about languages like Quechua that neither have a large amount of literary works, nor a significant number of native speakers? Are the models still able to generate reasonable translations? Secondly, what factors significantly influence the performance of LLMs in translation tasks? Is it to do with the prompts that we feed in or is it to do with the training data?</p><p>As per the results reported in the base paper, they perform a series of assessments on 8 LLMs that include English-centric ones like OPT, LLaMA2, LLaMA2-Chat, Falcon, and multilingual LLMs such as XGLM, BLOOMZ, ChatGPT, and GPT-4. It considers a broad spectrum of 102 languages across 606 translation directions. Their findings reveal improvements in the multilingual translation capabilities of LLMs, with GPT-4 achieving considerably high performance. Throughout this article, we focus on the working patterns of LLMs uncovered by the base paper and present our conclusions on the strengths and weaknesses of LLMs on MMT tasks.</p><h2 id="limitations-with-the-existing-state-of-machine-translation-using-llms" tabindex="-1"><a class="header-anchor" href="#limitations-with-the-existing-state-of-machine-translation-using-llms" aria-hidden="true">#</a> Limitations with the Existing State of Machine Translation using LLMs</h2><p>While the study highlights the impressive capabilities of LLMs, it&#39;s essential to acknowledge certain challenges and issues associated with machine translation systems, particularly the ones that use LLMs. One main challenge is that LLMs exhibit unbalanced translation capabilities across languages. They tend to perform better in translating into English than into non-English languages. Furthermore, GPT-4 faces greater challenges in French-centric and Chinese-centric translations, emphasizing the need for more balanced capabilities.</p><p>Another main challenge is the challenges caused by the underlying representations of language models. They can unknowingly propagate biases present in their training data to downstream tasks. If the training data contains biases, the language model may exhibit biased behavior, potentially reinforcing stereotypes or prejudices. Despite their linguistic power, language models may lack true comprehension and understanding. They generate responses based on patterns learned during training but may not truly grasp the underlying meaning. This can lead to responses that sound plausible but are factually incorrect or nonsensical.</p><p>Lastly, language models heavily rely on the data they are trained on. If there are language subtleties or cultural contexts not well-represented in the training data, the model may struggle with accurate translations in those areas. In the context of handling data, these models may inadvertently manage sensitive information improperly. There&#39;s a risk that language models might generate responses that reveal private or sensitive information, raising concerns about privacy and data security.</p><p>Addressing these challenges is crucial for the responsible development and deployment of large language models, ensuring they positively contribute to diverse applications without unintended consequences or biases.</p><h2 id="methodology" tabindex="-1"><a class="header-anchor" href="#methodology" aria-hidden="true">#</a> Methodology</h2><p>The primary focus of any MMT task is to gather data, with Flores-101 being a popular choice, as expressed by most researchers. No different from this is the base paper\u2019s strategy to benchmark the LLMs on the Flores-101 dataset, assessing various model qualities across a wide array of languages. This dataset is specifically curated for language translation tasks and it consists of source-target pairs across 101 languages, including low-resource languages such as Swahili and Quechua.</p><p><img src="https://github.com/sanjana-moudgalya/mnlp_blog/blob/main/MMT-LLM/ICL.png?raw=true" alt="alt text"> To evaluate the model\u2019s performance, the base paper employs the popular method of in-context learning (ICL). This is a technique used in machine learning where a system learns and improves by studying and understanding specific examples provided in the prompt[Figure 3]. This can be related to how humans learn in a real-world situation by observing examples of the task. For example, let\u2019s consider one wants to learn how to ride a bicycle. Traditional learning might involve reading a manual or watching videos on how to ride a bicycle. But according to ICL, you learn by actually getting on a bicycle and practicing in a safe and open area. Initially, you might struggle with balance, but with practice, you gradually understand how to maintain stability, pedal, steer, and eventually ride confidently. Similarly, this study uses ICL and considers various factors, including in-context examples, templates, and the impact of different languages on translation.</p><p>Using this strategy, the authors compare the performance of eight different LLMs (OPT, LLaMA2, LLaMA2-Chat, Falcon, XGLM, BLOOMZ, ChatGPT, and GPT-4) with three supervised baselines (M2M-100, NLLB, Google Translator). While LLMs excel in machine translation due to their diverse training data and flexible architectures, enabling them to grasp complex linguistic features, supervised baselines rely on curated datasets or specific task-oriented training, making them more suitable for MMT. In this paper, the authors explore the relationship between translation performance and the pre-training corpus size of LLMs, uncovering some fascinating results.</p><p>Now that we have an understanding of the language models and the translation task, we will look into methods that might be useful to evaluate the task. While a commonly used score to evaluate MMT models is the BLEU (Bilingual Evaluation Understudy) score, an alternative to this with fairer evaluation for low-resource languages is the SpBLEU score. It counts matching words/phrases between the translated sentences and reference sentences. It then calculates precision based on these matches and penalizes the length to ensure a reasonable length of translations. In addition to these steps, SpBLEU uses text that is tokenized using the language-independent SentencePiece library. On the other hand, COMET and SEScore use a neural network to evaluate the translation to emphasize semantic similarity and sentence-level context. This article focuses on the three above-mentioned metrics, SpBLEU, COMET, and SEScore to provide a comprehensive comparison of LLMs with strong supervised baselines to reveal the gap between translation paradigms.</p><h2 id="analysis-of-using-llms-for-machine-translation" tabindex="-1"><a class="header-anchor" href="#analysis-of-using-llms-for-machine-translation" aria-hidden="true">#</a> Analysis of using LLMs for Machine Translation</h2><p>An in-depth analysis of the performance of various LLMs reveals key factors influencing their translation performance. The study focuses on resource efficiency, prompt template design, and the importance of cross-lingual examples. Here are the main findings:</p><h4 id="_1-pre-training-corpus-size" tabindex="-1"><a class="header-anchor" href="#_1-pre-training-corpus-size" aria-hidden="true">#</a> 1. Pre-training Corpus Size:</h4><p>LLMs can perform moderately well in a resource-efficient manner even on low-resource languages, like Catalan and Swahili. Even if the translation capabilities aren\u2019t as great as high-resource languages, LLMs demonstrate a great learning potential through In-Context Learning (ICL), highlighting the model&#39;s adaptability.</p><h4 id="_2-in-context-template" tabindex="-1"><a class="header-anchor" href="#_2-in-context-template" aria-hidden="true">#</a> 2. In-context Template:</h4><p>The choice of prompt template significantly impacts translation performance, with variations of up to 16 BLEU points. Surprisingly, even seemingly unreasonable templates can effectively guide LLM in generating quality translations. &quot;X = Y&quot; results in the highest average BLEU score, whereas &quot;[SRC]: X \\n [TGT]: Y&quot; achieves the lowest score, where X is the source sentence and Y is the target sentence. This shows us how dynamic seemingly similar templates can be with respect to performance. <br> Example of the prompt template that gave us the best results during reproduction: <br> &quot;X = Y&quot; format: \u201EWir haben jetzt 4 Monate alte M\xE4use, die Diabetes hatten und jetzt keinen mehr haben\u201C, f\xFCgte er hinzu. = &quot;We now have 4-month-old mice that are non-diabetic that used to be diabetic,&quot; he added.</p><h4 id="_3-cross-lingual-exemplar" tabindex="-1"><a class="header-anchor" href="#_3-cross-lingual-exemplar" aria-hidden="true">#</a> 3. Cross-lingual Exemplar:</h4><p><img src="https://github.com/sanjana-moudgalya/mnlp_blog/blob/main/MMT-LLM/Cross-lingual Examplars.png?raw=true" alt="alt text"> Cross-lingual examples in the prompts prove beneficial for certain translation directions, especially for those involving low-resource languages (such as Quechua-English), showcasing potential versatility. The selection of semantically related examples does not necessarily enhance performance compared to randomly picked examples. LLM learns core translation features through examples, emphasizing the importance of context and diversity. However, it is also important to notice the correlation of performance with the number of examples provided. BLEU score improves up to 8 examples, plateaus till 32, and then gradually declines [Figure 4].</p><h4 id="_4-translation-granularity" tabindex="-1"><a class="header-anchor" href="#_4-translation-granularity" aria-hidden="true">#</a> 4. Translation Granularity:</h4><p>Word-level and document-level examples negatively affect LLMs\u2019 performance, emphasizing the need for appropriate granularity in example selection. Diverse and contextually relevant examples contribute to better translation outcomes. For instance, consider a translation task where a document-level example is used to illustrate the context of a specific phrase. If this lengthy document contains various unrelated topics, the model might struggle to extract the relevant information for the translation, leading to inaccuracies or inconsistencies in the output. Conversely, a word-level example precisely targeting the phrase&#39;s context allows the model to focus on the specific linguistic nuances needed for accurate translation.</p><h4 id="_5-prompt-structure-impact" tabindex="-1"><a class="header-anchor" href="#_5-prompt-structure-impact" aria-hidden="true">#</a> 5. Prompt Structure Impact:</h4><p>The placement of examples within the prompt has a varying impact on LLM&#39;s behavior. Reversing examples at the end of the prompt consistently leads to poorer results, highlighting the significance of prompt structure. For instance, compare these two prompts: &quot;Explain the impact of prompt structure on LLM. Provide examples to illustrate your points.&quot; versus &quot;Offer examples that demonstrate how LLM&#39;s behavior varies due to prompt structure. Explain the importance of prompt structure.&quot; In the first prompt, by asking for examples after the directive to explain, the model receives a clear context for the examples, enabling it to generate more coherent and relevant responses. Conversely, the second prompt, requesting examples before an explanation, poses a challenge for the model to understand the context for those examples, often resulting in less cohesive or relevant outputs.</p><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary" aria-hidden="true">#</a> Summary</h2><p>In the broader context of multilingual machine translation, this paper evaluates popular LLMs, such as ChatGPT and GPT-4, on 102 languages and 606 directions. While acknowledging continuous improvements, challenges remain for low-resource languages. LLMs have a lot of strengths, including the ability to ignore instruction semantics during in-context learning and the effectiveness of cross-lingual examples for low-resource translations. The analysis suggests a promising future for LLMs in resource-efficient multilingual machine translation.</p><p>The authors of this paper have done a good job at maintaining the codebase. Their GitHub repository is reproducible with a fairly small amount of changes. To provide evidence to the limitations and analysis section in this blog, we have a few BLEU scores after reproducing this paper using the NLLB (600M parameter) model -</p><p>German-English &gt; 43.78426080060355 <br> Assamese-English &gt; 27.894121883466795 <br> English-Assamese &gt; 23.10996837152803 <br> Swahili-English &gt; 39.080203011245224 <br></p><p>We can see that the BLEU score is lesser for Assamese and Swahili (low-resource languages when compared to German). Another important point to note is that generating Assamese text results in a lower score compared to generating English text, mainly because learning the vocabulary (to generate, instead of interpret) of a low-resource language might be harder.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><p>[1] Zhu, Wenhao, et al. &quot;Multilingual machine translation with large language models: Empirical results and analysis.&quot; arXiv preprint arXiv:2304.04675 (2023) <br> [2] https://thegradient.pub/in-context-learning-in-context/ <br> [3] https://blog.ml6.eu/navigating-ethical-considerations-developing-and-deploying-large-language-models-llms-d44f3fcde626 <br> [4] https://www.fiverr.com/ilovhus/translate-you-from-a-language-you-choose-to-any-other-language <br></p>',34);function w(x,M){const t=h("ExternalLinkIcon");return i(),r("div",null,[u,d,g,o(" more "),e("p",null,[p,e("a",m,[f,n(t)])]),e("p",null,[b,e("a",y,[L,n(t)])]),v])}const k=s(c,[["render",w],["__file","index.html.vue"]]);export{k as default};
