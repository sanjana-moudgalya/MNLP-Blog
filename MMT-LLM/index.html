<!DOCTYPE html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.51" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://sanjana-moudgalya.github.io/MMT-LLM/"><meta property="og:site_name" content="Evaluation of Machine Translations with Large Language Models"><meta property="og:title" content="Multilingual Machine Translation with Large Language Models"><meta property="og:type" content="article"><meta property="og:updated_time" content="2023-11-17T01:00:50.000Z"><meta property="og:locale" content="en-US"><meta property="article:author" content="Anusha Rao, Sanjana Moudgalya"><meta property="article:tag" content="Multilingual MT"><meta property="article:tag" content="Large Language Models"><meta property="article:tag" content="Evaluation with BLEU"><meta property="article:published_time" content="2023-11-16T00:00:00.000Z"><meta property="article:modified_time" content="2023-11-17T01:00:50.000Z"><title>Multilingual Machine Translation with Large Language Models | Evaluation of Machine Translations with Large Language Models</title><meta name="description" content="A Blog for Machine Learning, Natural Language Processing, and Data Mining">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d2025;
      }

      html,
      body {
        background-color: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="stylesheet" href="/assets/style.c54272bd.css">
    <link rel="modulepreload" href="/assets/app.5132c02c.js"><link rel="modulepreload" href="/assets/index.html.94304566.js"><link rel="modulepreload" href="/assets/_plugin-vue_export-helper.cdc0426e.js"><link rel="modulepreload" href="/assets/index.html.a2c36af8.js"><link rel="prefetch" href="/assets/index.html.268ddb49.js"><link rel="prefetch" href="/assets/404.html.50619a54.js"><link rel="prefetch" href="/assets/index.html.09c8363b.js"><link rel="prefetch" href="/assets/index.html.69899862.js"><link rel="prefetch" href="/assets/index.html.9fc605d2.js"><link rel="prefetch" href="/assets/index.html.c310ae00.js"><link rel="prefetch" href="/assets/index.html.c6690c9d.js"><link rel="prefetch" href="/assets/index.html.d1325f90.js"><link rel="prefetch" href="/assets/index.html.30fc159c.js"><link rel="prefetch" href="/assets/index.html.c7d6bb19.js"><link rel="prefetch" href="/assets/index.html.5eda7c46.js"><link rel="prefetch" href="/assets/index.html.45fc4a0a.js"><link rel="prefetch" href="/assets/index.html.6b055639.js"><link rel="prefetch" href="/assets/index.html.df0d19be.js"><link rel="prefetch" href="/assets/404.html.2e14b8b3.js"><link rel="prefetch" href="/assets/index.html.533d5446.js"><link rel="prefetch" href="/assets/index.html.45fdbc23.js"><link rel="prefetch" href="/assets/index.html.ffca5085.js"><link rel="prefetch" href="/assets/index.html.d59957f9.js"><link rel="prefetch" href="/assets/index.html.9df9ec4e.js"><link rel="prefetch" href="/assets/index.html.c69bc110.js"><link rel="prefetch" href="/assets/index.html.ce934061.js"><link rel="prefetch" href="/assets/index.html.a75ed039.js"><link rel="prefetch" href="/assets/index.html.202978b2.js"><link rel="prefetch" href="/assets/index.html.bb129224.js"><link rel="prefetch" href="/assets/index.html.1e740ec1.js"><link rel="prefetch" href="/assets/giscus.15440425.js"><link rel="prefetch" href="/assets/highlight.esm.d982e650.js"><link rel="prefetch" href="/assets/markdown.esm.832a189d.js"><link rel="prefetch" href="/assets/math.esm.a3f84b6f.js"><link rel="prefetch" href="/assets/notes.esm.3c361cb7.js"><link rel="prefetch" href="/assets/reveal.esm.b96f05d8.js"><link rel="prefetch" href="/assets/search.esm.80da4a02.js"><link rel="prefetch" href="/assets/zoom.esm.8514a202.js"><link rel="prefetch" href="/assets/photoswipe.esm.382b1873.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">Skip to content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><!--[--><header class="navbar"><div class="navbar-left"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><a href="/" class="brand"><img class="logo" src="/logo.svg" alt="Evaluation of Machine Translations with Large Language Models"><!----><span class="site-name hide-in-pad">Evaluation of Machine Translations with Large Language Models</span></a><!----></div><div class="navbar-center"><!----><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/" class="nav-link" aria-label="Blog Home"><span class="icon iconfont icon-home"></span>Blog Home<!----></a></div><div class="nav-item hide-in-mobile"><a href="/category/" class="nav-link" aria-label="Category"><span class="icon iconfont icon-categoryselected"></span>Category<!----></a></div><div class="nav-item hide-in-mobile"><a href="/tag/" class="nav-link" aria-label="Tags"><span class="icon iconfont icon-tag"></span>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a href="/timeline/" class="nav-link" aria-label="Timeline"><span class="icon iconfont icon-time"></span>Timeline<!----></a></div></nav><!----></div><div class="navbar-right"><!----><!----><div class="nav-item"><a class="repo-link" href="https://github.com/sanjana-moudgalya/mnlp_blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!----><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow left"></span></div><aside class="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->Multilingual Machine Translation with Large Language Models</h1><div class="page-info"><span class="author-info" aria-label="Author🖊" data-balloon-pos="down" localizeddate="November 16, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="author-item">Anusha Rao, Sanjana Moudgalya</span></span><span property="author" content="Anusha Rao, Sanjana Moudgalya"></span></span><!----><span class="date-info" aria-label="Writing Date📅" data-balloon-pos="down" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span>November 16, 2023</span><meta property="datePublished" content="2023-11-16T00:00:00.000Z"></span><span class="category-info" aria-label="Category🌈" data-balloon-pos="down" localizeddate="November 16, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><ul class="categories-wrapper"><li class="category category4 clickable" role="navigation">MT</li><meta property="articleSection" content="MT"></ul></span><span aria-label="Tag🏷" data-balloon-pos="down" localizeddate="November 16, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><ul class="tags-wrapper"><li class="tag tag7 clickable" role="navigation">Multilingual MT</li><li class="tag tag8 clickable" role="navigation">Large Language Models</li><li class="tag tag6 clickable" role="navigation">Evaluation with BLEU</li></ul><meta property="keywords" content="Multilingual MT,Large Language Models,Evaluation with BLEU"></span><span class="reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down" localizeddate="November 16, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 7 min</span><meta property="timeRequired" content="PT7M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><div class="toc-header">On This Page</div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/MMT-LLM/#introduction" class="router-link-active router-link-exact-active toc-link level2">Introduction</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/MMT-LLM/#limitations-with-the-existing-state-of-machine-translation-using-llms" class="router-link-active router-link-exact-active toc-link level2">Limitations with the Existing State of Machine Translation using LLMs</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/MMT-LLM/#methodology" class="router-link-active router-link-exact-active toc-link level2">Methodology</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/MMT-LLM/#analysis-of-using-llms-for-machine-translation" class="router-link-active router-link-exact-active toc-link level2">Analysis of using LLMs for Machine Translation</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/MMT-LLM/#summary" class="router-link-active router-link-exact-active toc-link level2">Summary</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/MMT-LLM/#references" class="router-link-active router-link-exact-active toc-link level2">References</a></li><!----><!--]--></ul></div></aside></div><!----><div class="theme-hope-content"><p>To all those tech enthusiasts out there, here’s yet another advancement of Large Language Models (LLM) that’s worth a read. This article explores how powerful these models are in language translation tasks by presenting a summary of evaluations on 8 LLMs including but not limited to Chat-GPT, GPT-4, and Google Translate.</p><!-- more --><p>Paper: <a href="https://arxiv.org/pdf/2304.04675.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2304.04675.pdf<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Code: <a href="https://github.com/NJUNLP/MMT-LLM" target="_blank" rel="noopener noreferrer">https://github.com/NJUNLP/MMT-LLM<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><p><img src="https://github.com/sanjana-moudgalya/mnlp_blog/blob/main/MMT-LLM/Introduction.png?raw=true" alt="alt text"> Effective communication is universally recognized as a key to success. But what about the challenges posed by linguistic barriers? Imagine being unable to articulate your thoughts without understanding the language of the person you&#39;re talking to. Wouldn&#39;t it be incredible if language was no longer a barrier, enabling seamless communication with anyone across the globe? Sounds crazy, right? But might not be that far away from reality [Figure 1], and all credit to the advancement in Large Language Models(LLMs).</p><p>Before we delve deeper into translation tasks, let&#39;s first understand what LLMs are. Although you might not be familiar with their workings or design, chances are you&#39;ve encountered them in various forms. For instance, Chat-GPT - does this sound familiar? They are smart systems powered by AI that have learned from heaps of text from all around the world, making them one of the most powerful models out there [Figure 2]. Their training corpus consists of multiple languages, allowing them to master not only their grammar but also the ability to understand the semantics of a language. Thus making them ideal for Multilingual Machine Translation (MMT) tasks.</p><p><img src="https://github.com/sanjana-moudgalya/mnlp_blog/blob/main/MMT-LLM/LLMs.png?raw=true" alt="alt text"> Now that we know what these models are capable of, let&#39;s evaluate their true abilities in language translation. Based on the base paper, we attempt to answer two main questions. Firstly, how effectively are LLMs able to translate text across numerous languages? For languages like English, it is very easy to find training data since it is widely spoken. But what about languages like Quechua that neither have a large amount of literary works, nor a significant number of native speakers? Are the models still able to generate reasonable translations? Secondly, what factors significantly influence the performance of LLMs in translation tasks? Is it to do with the prompts that we feed in or is it to do with the training data?</p><p>As per the results reported in the base paper, they perform a series of assessments on 8 LLMs that include English-centric ones like OPT, LLaMA2, LLaMA2-Chat, Falcon, and multilingual LLMs such as XGLM, BLOOMZ, ChatGPT, and GPT-4. It considers a broad spectrum of 102 languages across 606 translation directions. Their findings reveal improvements in the multilingual translation capabilities of LLMs, with GPT-4 achieving considerably high performance. Throughout this article, we focus on the working patterns of LLMs uncovered by the base paper and present our conclusions on the strengths and weaknesses of LLMs on MMT tasks.</p><h2 id="limitations-with-the-existing-state-of-machine-translation-using-llms" tabindex="-1"><a class="header-anchor" href="#limitations-with-the-existing-state-of-machine-translation-using-llms" aria-hidden="true">#</a> Limitations with the Existing State of Machine Translation using LLMs</h2><p>The research paper does a great job of scouting out and analyzing the strengths of LLMs in language translation tasks. A few include:</p><ul><li><strong>Ignoring Instruction Semantics</strong>: LLMs can surprisingly ignore instruction semantics when given in-context examples.</li><li><strong>Cross-Lingual Exemplars</strong>: Examples from different languages can provide better task guidance for low-resource translation than languages in the same resource group.</li><li><strong>Resource-Efficient Translation Acquisition</strong>: LLMs can acquire translation ability in a resource-efficient way, generating moderate translation even for zero-resource languages.</li></ul><p>While this research paper highlights the impressive capabilities of LLMs, it&#39;s essential to acknowledge certain challenges and issues associated with this approach. One main challenge is that LLMs exhibit unbalanced translation capabilities across languages. They tend to perform better in translating into English than into non-English languages. Furthermore, GPT-4 faces greater challenges in French-centric and Chinese-centric translations, emphasizing the need for more balanced capabilities.</p><p>Another main challenge is the challenges caused by the underlying representations of language models. They can unknowingly propagate biases present in their training data to downstream tasks. If the training data contains biases, the language model may exhibit biased behavior, potentially reinforcing stereotypes or prejudices. Despite their linguistic power, language models may lack true comprehension and understanding. They generate responses based on patterns learned during training but may not truly grasp the underlying meaning. This can lead to responses that sound plausible but are factually incorrect or nonsensical.</p><p>Lastly, language models heavily rely on the data they are trained on. If there are language subtleties or cultural contexts not well-represented in the training data, the model may struggle with accurate translations in those areas. In the context of handling data, these models may inadvertently manage sensitive information improperly. There&#39;s a risk that language models might generate responses that reveal private or sensitive information, raising concerns about privacy and data security.</p><p>Addressing these challenges is crucial for the responsible development and deployment of large language models, ensuring they positively contribute to diverse applications without unintended consequences or biases.</p><h2 id="methodology" tabindex="-1"><a class="header-anchor" href="#methodology" aria-hidden="true">#</a> Methodology</h2><p>The primary focus of any MMT task is to gather data, with Flores-101 being a popular choice, as expressed by most researchers. No different from this is the base paper’s strategy to benchmark the LLMs on the Flores-101 dataset, assessing various model qualities across a wide array of languages. This dataset is specifically curated for language translation tasks and it consists of source-target pairs across 101 languages, including low-resource languages such as Swahili and Quechua.</p><p><img src="https://github.com/sanjana-moudgalya/mnlp_blog/blob/main/MMT-LLM/ICL.png?raw=true" alt="alt text"> To evaluate the model’s performance, the base paper employs the popular method of in-context learning (ICL). This is a technique used in machine learning where a system learns and improves by studying and understanding specific examples provided in the prompt[Figure 3]. This can be related to how humans learn in a real-world situation by observing examples of the task. For example, let’s consider one wants to learn how to ride a bicycle. Traditional learning might involve reading a manual or watching videos on how to ride a bicycle. But according to ICL, you learn by actually getting on a bicycle and practicing in a safe and open area. Initially, you might struggle with balance, but with practice, you gradually understand how to maintain stability, pedal, steer, and eventually ride confidently. Similarly, this study uses ICL and considers various factors, including in-context examples, templates, and the impact of different languages on translation.</p><p>Using this strategy, the authors compare the performance of eight different LLMs (OPT, LLaMA2, LLaMA2-Chat, Falcon, XGLM, BLOOMZ, ChatGPT, and GPT-4) with three supervised baselines (M2M-100, NLLB, Google Translator). While LLMs excel in machine translation due to their diverse training data and flexible architectures, enabling them to grasp complex linguistic features, supervised baselines rely on curated datasets or specific task-oriented training, making them more suitable for MMT. In this paper, the authors explore the relationship between translation performance and the pre-training corpus size of LLMs, uncovering some fascinating results.</p><p>Now that we have an understanding of the language models and the translation task, we will look into methods that might be useful to evaluate the task. While a commonly used score to evaluate MMT models is the BLEU (Bilingual Evaluation Understudy) score, an alternative to this with fairer evaluation for low-resource languages is the SpBLEU score. It counts matching words/phrases between the translated sentences and reference sentences. It then calculates precision based on these matches and penalizes the length to ensure a reasonable length of translations. In addition to these steps, SpBLEU uses text that is tokenized using the language-independent SentencePiece library. On the other hand, COMET and SEScore use a neural network to evaluate the translation to emphasize semantic similarity and sentence-level context. This article focuses on the three above-mentioned metrics, SpBLEU, COMET, and SEScore to provide a comprehensive comparison of LLMs with strong supervised baselines to reveal the gap between translation paradigms.</p><h2 id="analysis-of-using-llms-for-machine-translation" tabindex="-1"><a class="header-anchor" href="#analysis-of-using-llms-for-machine-translation" aria-hidden="true">#</a> Analysis of using LLMs for Machine Translation</h2><p>An in-depth analysis of the performance of various LLMs reveals key factors influencing their translation performance. The study focuses on resource efficiency, prompt template design, and the importance of cross-lingual examples. Here are the main findings:</p><h4 id="_1-pre-training-corpus-size" tabindex="-1"><a class="header-anchor" href="#_1-pre-training-corpus-size" aria-hidden="true">#</a> 1. Pre-training Corpus Size:</h4><p>LLMs can perform moderately well in a resource-efficient manner even on low-resource languages, like Catalan and Swahili. Even if the translation capabilities aren’t as great as high-resource languages, LLMs demonstrate a great learning potential through In-Context Learning (ICL), highlighting the model&#39;s adaptability.</p><h4 id="_2-in-context-template" tabindex="-1"><a class="header-anchor" href="#_2-in-context-template" aria-hidden="true">#</a> 2. In-context Template:</h4><p>The choice of prompt template significantly impacts translation performance, with variations of up to 16 BLEU points. Surprisingly, even seemingly unreasonable templates can effectively guide LLM in generating quality translations. &quot;X = Y&quot; results in the highest average BLEU score, whereas &quot;[SRC]: X \n [TGT]: Y&quot; achieves the lowest score, where X is the source sentence and Y is the target sentence. This shows us how dynamic seemingly similar templates can be with respect to performance. <br> Example of the prompt template that gave us the best results during reproduction: <br> &quot;X = Y&quot; format: „Wir haben jetzt 4 Monate alte Mäuse, die Diabetes hatten und jetzt keinen mehr haben“, fügte er hinzu. = &quot;We now have 4-month-old mice that are non-diabetic that used to be diabetic,&quot; he added.</p><h4 id="_3-cross-lingual-exemplar" tabindex="-1"><a class="header-anchor" href="#_3-cross-lingual-exemplar" aria-hidden="true">#</a> 3. Cross-lingual Exemplar:</h4><p><img src="https://github.com/sanjana-moudgalya/mnlp_blog/blob/main/MMT-LLM/Cross-lingual Examplars.png?raw=true" alt="alt text"> Cross-lingual examples in the prompts prove beneficial for certain translation directions, especially for those involving low-resource languages (such as Quechua-English), showcasing potential versatility. The selection of semantically related examples does not necessarily enhance performance compared to randomly picked examples. LLM learns core translation features through examples, emphasizing the importance of context and diversity. However, it is also important to notice the correlation of performance with the number of examples provided. BLEU score improves up to 8 examples, plateaus till 32, and then gradually declines [Figure 4].</p><h4 id="_4-translation-granularity" tabindex="-1"><a class="header-anchor" href="#_4-translation-granularity" aria-hidden="true">#</a> 4. Translation Granularity:</h4><p>Word-level and document-level examples negatively affect LLMs’ performance, emphasizing the need for appropriate granularity in example selection. Diverse and contextually relevant examples contribute to better translation outcomes. It has been shown that duplicates in examples can adversely affect performance.</p><h4 id="_5-prompt-structure-impact" tabindex="-1"><a class="header-anchor" href="#_5-prompt-structure-impact" aria-hidden="true">#</a> 5. Prompt Structure Impact:</h4><p>The placement of examples within the prompt has a varying impact on LLM&#39;s behavior. Reversing examples at the end of the prompt consistently leads to poorer results, highlighting the significance of prompt structure.</p><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary" aria-hidden="true">#</a> Summary</h2><p>In the broader context of multilingual machine translation, this paper evaluates popular LLMs, such as ChatGPT and GPT-4, on 102 languages and 606 directions. While acknowledging continuous improvements, challenges remain for low-resource languages. LLMs have a lot of strengths, including the ability to ignore instruction semantics during in-context learning and the effectiveness of cross-lingual examples for low-resource translations. The analysis suggests a promising future for LLMs in resource-efficient multilingual machine translation.</p><p>The authors of this paper have done a good job at maintaining the codebase. Their GitHub repository is reproducible with a fairly small amount of changes. To provide evidence to the limitations and analysis section in this blog, we have a few BLEU scores after reproducing this paper using the NLLB (600M parameter) model -</p><p>German-English &gt; 43.78426080060355 <br> Assamese-English &gt; 27.894121883466795 <br> English-Assamese &gt; 23.10996837152803 <br> Swahili-English &gt; 39.080203011245224 <br></p><p>We can see that the BLEU score is lesser for Assamese and Swahili (low-resource languages when compared to German). Another important point to note is that generating Assamese text results in a lower score compared to generating English text, mainly because learning the vocabulary (to generate, instead of interpret) of a low-resource language might be harder.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><p>[1] Zhu, Wenhao, et al. &quot;Multilingual machine translation with large language models: Empirical results and analysis.&quot; arXiv preprint arXiv:2304.04675 (2023). [2] https://thegradient.pub/in-context-learning-in-context/ [3] https://blog.ml6.eu/navigating-ethical-considerations-developing-and-deploying-large-language-models-llms-d44f3fcde626 [4] https://www.fiverr.com/ilovhus/translate-you-from-a-language-you-choose-to-any-other-language</p></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/sanjana-moudgalya/mnlp_blog/edit/main/MMT-LLM/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item update-time"><span class="label">Last update: </span><span class="info">11/17/2023, 1:00:50 AM</span></div><div class="meta-item contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: 112517620+anushasa-rao@users.noreply.github.com">anushasa-rao</span>,<!--]--><!--[--><span class="contributor" title="email: 112443767+sanjana-moudgalya@users.noreply.github.com">sanjana-moudgalya</span>,<!--]--><!--[--><span class="contributor" title="email: smoudgal@andrew.cmu.edu">sanjana-moudgalya</span><!--]--><!--]--></div></footer><!----><div class="giscus-wrapper input-top" style="display:block;"><div style="text-align:center">Loading...</div></div><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer">Li Lab</div><div class="copyright">Copyright © 2023 Anusha Rao, Sanjana Moudgalya</div></footer><!--]--></div><!--]--><!----><!--]--></div>
    <script type="module" src="/assets/app.5132c02c.js" defer></script>
  </body>
</html>
